{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.onnx.symbolic_opset9 import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_LENGTH = 400"
   ],
   "id": "de3d1d71ae486834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imdb_ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\", \"tokenizer\", \"bert-base-uncased\"\n",
    ")"
   ],
   "id": "2f2e505ec53a4f32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_imdb(batch):\n",
    "    texts = []\n",
    "    for row in batch:\n",
    "        texts.append(row[\"text\"])\n",
    "\n",
    "    # (batch_size, MAX_LENGTH)\n",
    "    inputs = torch.LongTensor(\n",
    "        tokenizer(texts, padding=True, truncation=True, max_length=MAX_LENGTH).input_ids\n",
    "    )\n",
    "\n",
    "    last_words = []\n",
    "    for text in inputs:\n",
    "        last_word_token_idx = (text != tokenizer.pad_token_id).nonzero()[-2].item()\n",
    "        last_words.append(text[last_word_token_idx])\n",
    "    labels = torch.LongTensor(last_words)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    imdb_ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_imdb\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    imdb_ds[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_imdb\n",
    ")"
   ],
   "id": "6b90ce44ec5c415b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10_000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)"
   ],
   "id": "78c831d50e556454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # 1. Q, K, V 생성\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "\n",
    "        # 1.1 Reshape Q, K, V\n",
    "        Q = Q.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2. Attention 점수 계산\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3. Mask 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            # mask shape: [batch_size, 1, 1, seq_length]\n",
    "            # scores shape: [batch_size, n_heads, seq_length, seq_length]\n",
    "            # 마스크를 n_heads 차원으로 확장\n",
    "            mask = mask.repeat(1, self.n_heads, 1, 1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 4. Softmax 적용 및 Value 와 곱셈\n",
    "        attention_weights = self.softmax(scores)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 4.1 Transpose 및 Reshape\n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_length, self.d_model)\n",
    "        )\n",
    "\n",
    "        # 5. 최종 선형 변환\n",
    "        output = self.wo(output)\n",
    "\n",
    "        return output"
   ],
   "id": "3eb8f521f2a50786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.multi_head_attention(x, mask)\n",
    "        x1 = self.dropout1(x1)\n",
    "        x1 = self.layer_norm1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "        return self.layer_norm2(x2 + x1)"
   ],
   "id": "efb54967f6f6878f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, dff, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dff = dff\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(positional_encoding(max_len, d_model))\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.classification(x)\n",
    "        return x"
   ],
   "id": "94f467ef890394d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def accuracy(m, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(my_device), labels.to(my_device)\n",
    "\n",
    "        preds = m(inputs)\n",
    "        preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ],
   "id": "ab73170c6681fe8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = TextClassifier(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=32,\n",
    "    n_layers=5,\n",
    "    n_heads=4,\n",
    "    dff=32,\n",
    "    max_len=MAX_LENGTH,\n",
    ").to(my_device)"
   ],
   "id": "5d6fbddc7e914fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "cf4a414223f0e6c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_data_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(my_device), labels.to(my_device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predictions = outputs.squeeze()\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_data_loader)\n",
    "        test_acc = accuracy(model, test_data_loader)\n",
    "        print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ],
   "id": "8f2c5cd1a8872302",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
