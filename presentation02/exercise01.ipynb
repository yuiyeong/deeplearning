{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:00.508863Z",
     "start_time": "2024-09-26T13:04:59.292843Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:01.373574Z",
     "start_time": "2024-09-26T13:05:01.370423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_LENGTH = 400"
   ],
   "id": "573d196b13c642bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:08.793148Z",
     "start_time": "2024-09-26T13:05:03.429383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\", \"tokenizer\", \"bert-base-uncased\"\n",
    ")"
   ],
   "id": "897f9383b58aa458",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/joyuiyeong/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/Users/joyuiyeong/.pyenv/versions/deeplearning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IMDB 의 DataLoader",
   "id": "27ddcf6547cc0200"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:11.745198Z",
     "start_time": "2024-09-26T13:05:11.740953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_imdb(batch):\n",
    "    texts, labels = [], []\n",
    "    for row in batch:\n",
    "        texts.append(row[\"text\"])\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    texts = torch.LongTensor(\n",
    "        tokenizer(texts, padding=True, truncation=True, max_length=MAX_LENGTH).input_ids\n",
    "    )\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    imdb_ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_imdb\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    imdb_ds[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_imdb\n",
    ")"
   ],
   "id": "583805f3d1b51292",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Positional Encoding",
   "id": "846aa42d695bec7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:13.996652Z",
     "start_time": "2024-09-26T13:05:13.993282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10_000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)"
   ],
   "id": "af1356619480be88",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi Head Attention\n",
    "- Self-attention module을 MHA로 확장하기\n",
    "\n",
    "1. 기존의 $W_q, W_k, W_v$를 사용하여 $Q, K, V$를 생성합니다.\n",
    "    1. $Q, K, V \\in \\mathbb{R}^{S \\times D}$가 있을 때, 이를 $Q, K, V \\in \\mathbb{R}^{S \\times H \\times D’}$으로 reshape 해줍니다. 여기서 $H$는 `n_heads`라는 인자로 받아야 하고, $D$가 $H$로 나눠 떨어지는 값이여야 하는 제약 조건이 필요합니다. $D = H \\times D’$입니다.\n",
    "    2. $Q, K, V$를 $Q, K, V \\in \\mathbb{R}^{H \\times S \\times D’}$의 shape으로 transpose해줍니다.\n",
    "2. $A = QK^T/\\sqrt{D'} \\in \\mathbb{R}^{H \\times S \\times S}$를 기존의 self-attention과 똑같이 계산합니다.\n",
    "3. Mask를 더합니다. 기존과 $A$의 shape이 달라졌기 때문에 dimension을 어떻게 맞춰줘야할지 생각해줘야 합니다.\n",
    "4. $\\hat{x} = \\textrm{Softmax}(A)V \\in \\mathbb{R}^{H \\times S \\times D'}$를 계산해주고 transpose와 reshape을 통해 $\\hat{x} \\in \\mathbb{R}^{S \\times D}$의 shape으로 다시 만들어줍니다.\n",
    "5. 기존과 똑같이 $\\hat{x} = \\hat{x} W_o$를 곱해줘서 마무리 해줍니다."
   ],
   "id": "40e0e959c4df345e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:16.489214Z",
     "start_time": "2024-09-26T13:05:16.483840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # 1. Q, K, V 생성\n",
    "        Q = self.wq(x)\n",
    "        K = self.wk(x)\n",
    "        V = self.wv(x)\n",
    "\n",
    "        # 1.1 Reshape Q, K, V\n",
    "        Q = Q.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2. Attention 점수 계산\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3. Mask 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            # mask shape: [batch_size, 1, 1, seq_length]\n",
    "            # 마스크를 n_heads 차원으로 확장\n",
    "            mask = mask.repeat(1, self.n_heads, 1, 1)\n",
    "            # scores shape: [batch_size, n_heads, seq_length, seq_length]\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 4. Softmax 적용 및 Value 와 곱셈\n",
    "        attention_weights = self.softmax(scores)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 4.1 Transpose 및 Reshape\n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_length, self.d_model)\n",
    "        )\n",
    "\n",
    "        # 5. 최종 선형 변환\n",
    "        output = self.wo(output)\n",
    "\n",
    "        return output"
   ],
   "id": "e3dbfb1fd2cc1cee",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer Layer\n",
    "\n",
    "- 다음 내용을 추가하여 Improved Transformer Layer 를 만든다.\n",
    "    - Layer Normalization\n",
    "    - Dropout\n",
    "    - Residual Connection"
   ],
   "id": "7856e5b83cffb8af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:19.609135Z",
     "start_time": "2024-09-26T13:05:19.605103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.multi_head_attention(x, mask)\n",
    "        x1 = self.dropout1(x1)\n",
    "        x1 = self.layer_norm1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "        return self.layer_norm2(x2 + x1)"
   ],
   "id": "e2931307665ad928",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TextClassifier",
   "id": "1e61b322f3f0940f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:22.673837Z",
     "start_time": "2024-09-26T13:05:22.669150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, dff, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dff = dff\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(positional_encoding(max_len, d_model))\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.classification = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.classification(x)\n",
    "        return x"
   ],
   "id": "4456908bcf1be03f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:25.594956Z",
     "start_time": "2024-09-26T13:05:25.578713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def accuracy(m, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(my_device), labels.to(my_device)\n",
    "\n",
    "        preds = m(inputs)\n",
    "        # preds = torch.argmax(preds, dim=-1)\n",
    "        preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ],
   "id": "46ae6c6841b30ce5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 학습",
   "id": "fbac449126f83b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:27.813160Z",
     "start_time": "2024-09-26T13:05:27.764621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TextClassifier(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=32,\n",
    "    n_layers=5,\n",
    "    n_heads=4,\n",
    "    dff=32,\n",
    "    max_len=MAX_LENGTH,\n",
    ").to(my_device)"
   ],
   "id": "63d36471569cd847",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:29.000583Z",
     "start_time": "2024-09-26T13:05:28.672546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "5456b20cb2da1733",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-26T13:08:03.193017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_data_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(my_device), labels.to(my_device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predictions = outputs.squeeze()\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_data_loader)\n",
    "        test_acc = accuracy(model, test_data_loader)\n",
    "        print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ],
   "id": "24f305e0a12b76af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss: 144.99559339880943\n",
      "=========> Train acc: 0.882 | Test acc: 0.818\n",
      "Epoch   2 | Train Loss: 114.76658698916435\n",
      "=========> Train acc: 0.919 | Test acc: 0.832\n",
      "Epoch   3 | Train Loss: 90.99008486047387\n",
      "=========> Train acc: 0.947 | Test acc: 0.832\n",
      "Epoch   4 | Train Loss: 69.73941466212273\n",
      "=========> Train acc: 0.968 | Test acc: 0.838\n",
      "Epoch   5 | Train Loss: 52.38163839094341\n",
      "=========> Train acc: 0.976 | Test acc: 0.833\n",
      "Epoch   6 | Train Loss: 41.36907316930592\n",
      "=========> Train acc: 0.981 | Test acc: 0.827\n",
      "Epoch   7 | Train Loss: 33.79754772782326\n",
      "=========> Train acc: 0.986 | Test acc: 0.822\n",
      "Epoch   8 | Train Loss: 27.308424833230674\n",
      "=========> Train acc: 0.990 | Test acc: 0.826\n",
      "Epoch   9 | Train Loss: 24.63121843477711\n",
      "=========> Train acc: 0.991 | Test acc: 0.830\n",
      "Epoch  10 | Train Loss: 21.357082075905055\n",
      "=========> Train acc: 0.988 | Test acc: 0.827\n",
      "Epoch  11 | Train Loss: 19.383768710307777\n",
      "=========> Train acc: 0.993 | Test acc: 0.828\n",
      "Epoch  12 | Train Loss: 17.594352655112743\n",
      "=========> Train acc: 0.993 | Test acc: 0.828\n",
      "Epoch  13 | Train Loss: 19.90553431538865\n",
      "=========> Train acc: 0.994 | Test acc: 0.829\n",
      "Epoch  14 | Train Loss: 13.90827761287801\n",
      "=========> Train acc: 0.995 | Test acc: 0.824\n",
      "Epoch  15 | Train Loss: 15.173538187751547\n",
      "=========> Train acc: 0.991 | Test acc: 0.823\n",
      "Epoch  16 | Train Loss: 13.372797583928332\n",
      "=========> Train acc: 0.995 | Test acc: 0.827\n",
      "Epoch  17 | Train Loss: 13.3533332971856\n",
      "=========> Train acc: 0.987 | Test acc: 0.823\n",
      "Epoch  18 | Train Loss: 14.880401412257925\n",
      "=========> Train acc: 0.995 | Test acc: 0.825\n",
      "Epoch  19 | Train Loss: 13.087219124659896\n",
      "=========> Train acc: 0.995 | Test acc: 0.825\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1926ff8aa8c1451e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
