{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 간단한 Transformer 구현해보기\n",
    "- IMDB 데이터셋을 가지고, review 에 대해서 긍정인지 부정인지를 판별하는 모델을 만든다. "
   ],
   "id": "5e98c13310cc55a4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T10:09:57.259847Z",
     "start_time": "2024-09-26T10:09:56.389161Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer 준비",
   "id": "b41b292cef772ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:10:18.376657Z",
     "start_time": "2024-09-26T10:10:14.584373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\", \"tokenizer\", \"bert-base-uncased\"\n",
    ")"
   ],
   "id": "86d5f2448839a56f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/joyuiyeong/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/Users/joyuiyeong/.pyenv/versions/deeplearning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IMDB 에 대한 DataLoader 준비",
   "id": "1b30c177c9b508cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:10:30.237577Z",
     "start_time": "2024-09-26T10:10:30.233351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_len = 400\n",
    "\n",
    "\n",
    "def collate_imdb(batch):\n",
    "    texts, labels = [], []\n",
    "    for row in batch:\n",
    "        texts.append(row[\"text\"])\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    texts = torch.LongTensor(\n",
    "        tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids\n",
    "    )\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_imdb\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=64, shuffle=False, collate_fn=collate_imdb\n",
    ")"
   ],
   "id": "633392c302e02fa2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer 의 Encoder 구조\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    Input[(\"Input Tokens\")]\n",
    "    Embed[\"Embedding Layer\"]\n",
    "    PosEnc[\"Positional Encoding\"]\n",
    "    \n",
    "    subgraph \"Encoder Layer (Repeated N times)\"\n",
    "        direction TB\n",
    "        subgraph \"Multi-Head Attention\"\n",
    "            direction LR\n",
    "            Q[\"Q Linear\"]\n",
    "            K[\"K Linear\"]\n",
    "            V[\"V Linear\"]\n",
    "            QK{{\"Q * K^T / √dk\"}}\n",
    "            Softmax[\"Softmax\"]\n",
    "            AttOut[\"Attention Output\"]\n",
    "        end\n",
    "        \n",
    "        Add1((\"+ Add\"))\n",
    "        Norm1[\"Layer Norm\"]\n",
    "        \n",
    "        FF[\"Feed Forward Network\"]\n",
    "        \n",
    "        Add2((\"+ Add\"))\n",
    "        Norm2[\"Layer Norm\"]\n",
    "    end\n",
    "    \n",
    "    Output[(\"Output\")]\n",
    "    \n",
    "    Input --> Embed\n",
    "    Embed --> PosEnc\n",
    "    PosEnc --> Q & K & V\n",
    "    Q & K --> QK\n",
    "    QK --> Softmax\n",
    "    Softmax & V --> AttOut\n",
    "    \n",
    "    AttOut --> Add1\n",
    "    PosEnc -.-> Add1\n",
    "    Add1 --> Norm1\n",
    "    Norm1 --> FF\n",
    "    FF --> Add2\n",
    "    Norm1 -.-> Add2\n",
    "    Add2 --> Norm2\n",
    "    Norm2 --> Output\n",
    "```"
   ],
   "id": "db7a28c1d501ede6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Self-Attention 구현\n",
    "\n",
    "- Shape이 $(S, D)$인 embedding $x$가 주어졌을 때, self-attention은 다음과 같이 계산합니다:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    Input[(\"Input Sequence\")]\n",
    "    \n",
    "    subgraph \"Self-Attention\"\n",
    "        direction TB\n",
    "        QProj[\"Query Projection\"]\n",
    "        KProj[\"Key Projection\"]\n",
    "        VProj[\"Value Projection\"]\n",
    "        \n",
    "        MatMul1{{\"Matrix Multiplication\"}}\n",
    "        Scale[/\"Scale (÷ √dk)\"\\]\n",
    "        Mask[\"Apply Mask (optional)\"]\n",
    "        Softmax[\"Softmax\"]\n",
    "        MatMul2{{\"Matrix Multiplication\"}}\n",
    "    end\n",
    "    \n",
    "    Output[(\"Attention Output\")]\n",
    "    \n",
    "    Input --> QProj & KProj & VProj\n",
    "    QProj --> MatMul1\n",
    "    KProj --> KT[\"Transpose\"]\n",
    "    KT --> MatMul1\n",
    "    MatMul1 --> Scale\n",
    "    Scale --> Mask\n",
    "    Mask --> Softmax\n",
    "    Softmax --> MatMul2\n",
    "    VProj --> MatMul2\n",
    "    MatMul2 --> Output\n",
    "    \n",
    "    style Input fill:#f9f,stroke:#333,stroke-width:4px\n",
    "    style Output fill:#bbf,stroke:#333,stroke-width:4px\n",
    "    style QProj fill:#fdd\n",
    "    style KProj fill:#dfd\n",
    "    style VProj fill:#ddf\n",
    "    style MatMul1 fill:#ffd\n",
    "    style MatMul2 fill:#ffd\n",
    "    style Scale fill:#eff\n",
    "    style Mask fill:#ffe\n",
    "    style Softmax fill:#eef\n",
    "```\n",
    "\n",
    "$$\n",
    "\\begin{align*} Q, K, V &= xW_q, xW_k, xW_v \\in \\mathbb{R}^{S \\times D},\\\\ A &= \\textrm{Softmax}\\left(\\frac{QK^T}{\\sqrt{D}}, \\textrm{dim=1}\\right) \\in \\mathbb{R}^{S \\times S}, \\\\ \\hat{x}&=AV W_o \\in \\mathbb{R}^{S \\times D}. \\end{align*}\n",
    "$$\n",
    "\n",
    "- 여기서 $W_q, W_k, W_v, W_o \\in \\mathbb{R}^{D \\times D}$는 MLP에서 사용하는 weight matrix와 동일한 parameter들입니다. \n",
    "- 보시다시피 $Q$를 자기자신 $x$로 부터 뽑은 것을 제외하면 sequence-to-sequence와 동일합니다. 자기자신과 attention을 계산하여 처리하기 대문에 self-attention이라고 부릅니다."
   ],
   "id": "d6daf3981dd8bd02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # mask 는 실제 attention 계산에서 padding token 을 무시하기 위해 제공되는 tensor\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        score = torch.matmul(q, k.transpose(-2, -1))\n",
    "        score = score / math.sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            # -1e9 는 매우 작은 값으로, softmax 를 거치게 되면 0에 가까워져서 weight sum 과정에서 padding token 은 무시할 수 있게 됩니다.\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        result = torch.matmul(score, v)\n",
    "        result = self.dense(result)\n",
    "        return result"
   ],
   "id": "6207fe8cb52964a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 간단한 Transformer Layer\n",
    "- Self-Attention 층과 Feed-Forward 층만 있는 Transformer Layer 를 정의합니다."
   ],
   "id": "c7da9061e1240f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        self.sa = SelfAttention(input_dim, d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sa(x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ],
   "id": "5eff2beb56c5191d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding\n",
    "- Scaled Dot Product 인 Self-Attention 만 하면, token 의 위치 정보를 반영하지 못합니다.\n",
    "- 그래서 위치 정보도 넣어주기 위해 Positional Encoding 을 진행합니다.\n",
    "- `nn.Embedding`에서 나온 embedding 들에 다음과 같은 positional encoding 이라는 값을 더해줘 순서 정보를 주입합니다 \n",
    "\n",
    "$$\n",
    "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
    "$$\n",
    "\n",
    "- 여기서 $(S, D)$는 입력 embedding $x$의 shape입니다. \n",
    "\n",
    "- 결과적으로 다음과 같이 순서 정보를 주입합니다:\n",
    "\n",
    "$$\n",
    "x_{\\textrm{positional}} = x + PE.\n",
    "$$\n",
    "\n",
    "- Transformer의 positional encoding을 주기함수를 쓰고, 각 차원마다 다른 주기함수를 쓰는 것 같습니다. 왜 이렇게 주기함수를 빈번하게 사용하는건가요?\n",
    "    - 위와 같이 positional encoding을 설정한 이유는 다음과 같이 정리할 수 있습니다.\n",
    "        1. **Bound된 positional encoding 값:** 주기함수를 쓰면 값들이 bound되기 때문에 아주 큰 값이 embedding에 더해지는 것을 방지할 수 있습니다.\n",
    "        2. **위치마다 다른 positional encoding 값:** 기본적으로 positional encoding은 token 위치마다 다른 값을 가져야 합니다. 차원마다 다른 주기함수를 사용하여 이를 보장해줍니다.\n",
    "        3. **$S$와 무관한 positional encoding 값:** 우리가 궁금한건 token 사이의 상대적인 위치 정보이지, 절대적인 정보가 아닙니다. 그래서 $S$와 무관한 positional encoding이 필요합니다.\n",
    "\n",
    "- 결과적으로 만들어진 positional encoding $PE$를 가지고 다음과 element-wise 덧셈 연산을 사용하여 순서 정보를 주입합니다.\n",
    "- 이렇게 위치 정보를 미리 계산해서 넣으면, 이 정보에 대해서는 학습을 진행하지 않고 계산된 값을 사용합니다."
   ],
   "id": "9ce6bdbe76f60450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:47:44.025371Z",
     "start_time": "2024-09-26T10:47:44.021275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10_000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)"
   ],
   "id": "9d8b3fa1d86b75ce",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 모델 정의\n",
    "- 위에서 정의한 SelfAttention, TransformerLayer, positional_encoding 을 사용하여, model 을 정의합니다. "
   ],
   "id": "b3a5548663a9c29d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:10:34.114595Z",
     "start_time": "2024-09-26T10:10:34.109353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(\n",
    "            positional_encoding(max_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = x == tokenizer.pad_token_id\n",
    "        mask = mask[:, None, :]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.classification(x)\n",
    "        return x"
   ],
   "id": "d704f663835566a8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:10:35.367471Z",
     "start_time": "2024-09-26T10:10:34.950138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "model = TextClassifier(vocab_size=len(tokenizer), d_model=32, n_layers=2, dff=32).to(\n",
    "    device\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ],
   "id": "3ff7739cefa256c7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:10:36.786196Z",
     "start_time": "2024-09-26T10:10:36.618243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(m, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        preds = m(inputs)\n",
    "        # preds = torch.argmax(preds, dim=-1)\n",
    "        preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ],
   "id": "8ea6f74d2d09ec67",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T10:30:12.453894Z",
     "start_time": "2024-09-26T10:10:38.012116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_data_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        preds = model(inputs)[..., 0]\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_data_loader)\n",
    "        test_acc = accuracy(model, test_data_loader)\n",
    "        print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ],
   "id": "a6212a5f662aff41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 224.93581557273865\n",
      "=========> Train acc: 0.788 | Test acc: 0.756\n",
      "Epoch   1 | Train Loss: 171.86157739162445\n",
      "=========> Train acc: 0.841 | Test acc: 0.791\n",
      "Epoch   2 | Train Loss: 146.20741969347\n",
      "=========> Train acc: 0.868 | Test acc: 0.805\n",
      "Epoch   3 | Train Loss: 127.01036885380745\n",
      "=========> Train acc: 0.892 | Test acc: 0.808\n",
      "Epoch   4 | Train Loss: 109.9486108198762\n",
      "=========> Train acc: 0.911 | Test acc: 0.805\n",
      "Epoch   5 | Train Loss: 93.10522639751434\n",
      "=========> Train acc: 0.933 | Test acc: 0.809\n",
      "Epoch   6 | Train Loss: 76.56934222206473\n",
      "=========> Train acc: 0.947 | Test acc: 0.805\n",
      "Epoch   7 | Train Loss: 66.30307236686349\n",
      "=========> Train acc: 0.959 | Test acc: 0.806\n",
      "Epoch   8 | Train Loss: 53.81126401014626\n",
      "=========> Train acc: 0.969 | Test acc: 0.807\n",
      "Epoch   9 | Train Loss: 42.767592184245586\n",
      "=========> Train acc: 0.977 | Test acc: 0.802\n",
      "Epoch  10 | Train Loss: 33.90505462652072\n",
      "=========> Train acc: 0.982 | Test acc: 0.803\n",
      "Epoch  11 | Train Loss: 26.080242573283613\n",
      "=========> Train acc: 0.985 | Test acc: 0.799\n",
      "Epoch  12 | Train Loss: 23.747224462218583\n",
      "=========> Train acc: 0.983 | Test acc: 0.798\n",
      "Epoch  13 | Train Loss: 20.589894138043746\n",
      "=========> Train acc: 0.989 | Test acc: 0.795\n",
      "Epoch  14 | Train Loss: 18.19367431802675\n",
      "=========> Train acc: 0.993 | Test acc: 0.799\n",
      "Epoch  15 | Train Loss: 15.921969600720331\n",
      "=========> Train acc: 0.996 | Test acc: 0.798\n",
      "Epoch  16 | Train Loss: 12.75600323319668\n",
      "=========> Train acc: 0.986 | Test acc: 0.789\n",
      "Epoch  17 | Train Loss: 13.519065420492552\n",
      "=========> Train acc: 0.990 | Test acc: 0.798\n",
      "Epoch  18 | Train Loss: 10.754945853317622\n",
      "=========> Train acc: 0.995 | Test acc: 0.796\n",
      "Epoch  19 | Train Loss: 9.648801995965187\n",
      "=========> Train acc: 0.995 | Test acc: 0.797\n",
      "Epoch  20 | Train Loss: 9.691239046311239\n",
      "=========> Train acc: 0.994 | Test acc: 0.794\n",
      "Epoch  21 | Train Loss: 9.801234877551906\n",
      "=========> Train acc: 0.995 | Test acc: 0.790\n",
      "Epoch  22 | Train Loss: 9.792091702111065\n",
      "=========> Train acc: 0.994 | Test acc: 0.793\n",
      "Epoch  23 | Train Loss: 7.974546240118798\n",
      "=========> Train acc: 0.994 | Test acc: 0.788\n",
      "Epoch  24 | Train Loss: 8.463017052912619\n",
      "=========> Train acc: 0.996 | Test acc: 0.798\n",
      "Epoch  25 | Train Loss: 9.806178616709076\n",
      "=========> Train acc: 0.995 | Test acc: 0.791\n",
      "Epoch  26 | Train Loss: 5.464490217011189\n",
      "=========> Train acc: 0.994 | Test acc: 0.786\n",
      "Epoch  27 | Train Loss: 7.932184845485608\n",
      "=========> Train acc: 0.998 | Test acc: 0.799\n",
      "Epoch  28 | Train Loss: 6.8273089184076525\n",
      "=========> Train acc: 0.997 | Test acc: 0.793\n",
      "Epoch  29 | Train Loss: 6.121511636811192\n",
      "=========> Train acc: 0.998 | Test acc: 0.799\n",
      "Epoch  30 | Train Loss: 5.122940487461165\n",
      "=========> Train acc: 0.998 | Test acc: 0.793\n",
      "Epoch  31 | Train Loss: 6.221661582727393\n",
      "=========> Train acc: 0.997 | Test acc: 0.794\n",
      "Epoch  32 | Train Loss: 7.2247861088690115\n",
      "=========> Train acc: 0.993 | Test acc: 0.789\n",
      "Epoch  33 | Train Loss: 7.705163370235823\n",
      "=========> Train acc: 0.998 | Test acc: 0.791\n",
      "Epoch  34 | Train Loss: 5.514557140690158\n",
      "=========> Train acc: 0.997 | Test acc: 0.792\n",
      "Epoch  35 | Train Loss: 3.5273347641887085\n",
      "=========> Train acc: 0.998 | Test acc: 0.790\n",
      "Epoch  36 | Train Loss: 6.1149139927947544\n",
      "=========> Train acc: 0.997 | Test acc: 0.790\n",
      "Epoch  37 | Train Loss: 5.9623430736101\n",
      "=========> Train acc: 0.998 | Test acc: 0.793\n",
      "Epoch  38 | Train Loss: 4.532383004843723\n",
      "=========> Train acc: 0.998 | Test acc: 0.795\n",
      "Epoch  39 | Train Loss: 4.411948925066099\n",
      "=========> Train acc: 0.996 | Test acc: 0.786\n",
      "Epoch  40 | Train Loss: 5.113162343848671\n",
      "=========> Train acc: 0.997 | Test acc: 0.787\n",
      "Epoch  41 | Train Loss: 4.429315403289365\n",
      "=========> Train acc: 0.998 | Test acc: 0.791\n",
      "Epoch  42 | Train Loss: 4.579242664840422\n",
      "=========> Train acc: 0.999 | Test acc: 0.794\n",
      "Epoch  43 | Train Loss: 5.669925452135885\n",
      "=========> Train acc: 0.999 | Test acc: 0.793\n",
      "Epoch  44 | Train Loss: 3.1664688409946393\n",
      "=========> Train acc: 0.999 | Test acc: 0.793\n",
      "Epoch  45 | Train Loss: 3.9548051613510324\n",
      "=========> Train acc: 0.997 | Test acc: 0.787\n",
      "Epoch  46 | Train Loss: 5.92381268959798\n",
      "=========> Train acc: 0.999 | Test acc: 0.795\n",
      "Epoch  47 | Train Loss: 3.651431124555529\n",
      "=========> Train acc: 0.999 | Test acc: 0.793\n",
      "Epoch  48 | Train Loss: 3.2039368530968204\n",
      "=========> Train acc: 0.999 | Test acc: 0.793\n",
      "Epoch  49 | Train Loss: 4.663189938746655\n",
      "=========> Train acc: 0.998 | Test acc: 0.793\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
