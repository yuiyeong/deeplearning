{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전이 학습 (Transfer Learning)\n",
    "- 한 도메인에서 학습된 지식을 다른 관련 도메인에 적용하는 기술\n",
    "- 모델을 학습시키는 데 있어서 데이터가 부족하거나 모델을 새로 만들기 어려울 때, 이미 학습된 모델을 사용하여 자신의 문제에 맞게 재학습하는 방법\n",
    "\n",
    "##  주요 전이학습 기법\n",
    "\n",
    "### 1. 파인 튜닝 (Fine-tuning)\n",
    "- 사전 훈련된 모델의 일부 또는 전체 가중치를 새로운 작업에 맞게 조정합니다.\n",
    "- 새로운 데이터셋으로 모델을 추가 학습시키는 방식입니다.\n",
    "- 장점: 상대적으로 간단하고 효과적입니다.\n",
    "- 단점: 대규모 모델의 경우 많은 컴퓨팅 자원이 필요할 수 있습니다.\n",
    "\n",
    "### 2. LoRA (Low-Rank Adaptation)\n",
    "- 대규모 언어 모델을 효율적으로 fine-tuning하기 위해 개발되었습니다.\n",
    "- 모델의 가중치 매트릭스에 저랭크 행렬을 더하는 방식으로 작동합니다.\n",
    "- 장점: 매우 적은 수의 파라미터만 학습하므로 효율적입니다.\n",
    "- 특히 대규모 언어 모델에서 유용합니다.\n",
    "\n",
    "### 3. 특징 추출 (Feature Extraction)\n",
    "- 사전 훈련된 모델의 일부 레이어를 고정하고, 마지막 레이어만 새로운 작업에 맞게 학습시킵니다.\n",
    "- 장점: 계산 비용이 적고, 과적합 위험이 낮습니다.\n",
    "- 단점: 파인 튜닝에 비해 성능이 떨어질 수 있습니다.\n",
    "\n",
    "### 4. 프롬프트 튜닝 (Prompt Tuning)\n",
    "- 모델의 가중치는 고정하고, 입력 프롬프트만을 최적화합니다.\n",
    "- 장점: 매우 적은 수의 파라미터만 학습하므로 효율적입니다.\n",
    "- 특히 대규모 언어 모델에서 유용합니다.\n",
    "\n",
    "### 5. 어댑터 (Adapters)\n",
    "- 모델의 각 레이어에 작은 신경망(어댑터)을 추가합니다.\n",
    "- 기존 모델 파라미터는 고정하고 어댑터만 학습합니다.\n",
    "- 장점: 모델 아키텍처를 크게 변경하지 않고도 새로운 작업에 적응할 수 있습니다.\n",
    "\n",
    "### 6. P-tuning\n",
    "- 프롬프트 튜닝의 변형으로, 연속적인 프롬프트 임베딩을 학습합니다.\n",
    "- 장점: 프롬프트 튜닝보다 더 유연하고 효과적일 수 있습니다.\n",
    "\n",
    "\n",
    "- 이러한 방법들은 각각의 장단점이 있으며, 작업의 특성, 사용 가능한 컴퓨팅 자원, 데이터셋의 크기 등에 따라 선택됩니다. \n",
    "- 최근에는 대규모 언어 모델의 등장으로 LoRA, 프롬프트 튜닝 등의 효율적인 방법들이 주목받고 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning 의 주요 두 가지 방법\n",
    "### 전체 모델 학습 (Full Fine-tuning)\n",
    "\n",
    "- 특징\n",
    "    - 사전 훈련된 모델의 모든 파라미터를 업데이트합니다.\n",
    "    - 가장 유연한 방법으로, 모델 전체를 새로운 작업에 맞게 조정할 수 있습니다.\n",
    "\n",
    "- 장점\n",
    "    - 높은 성능: 모든 파라미터를 조정하므로 잠재적으로 최고의 성능을 얻을 수 있습니다.\n",
    "    - 유연성: 새로운 작업에 완전히 적응할 수 있습니다.\n",
    "\n",
    "- 단점\n",
    "    - 계산 비용: 모든 파라미터를 업데이트하므로 계산 비용이 높습니다.\n",
    "    - 과적합 위험: 특히 작은 데이터셋에서 과적합 위험이 있습니다.\n",
    "    - 메모리 요구: 큰 모델의 경우 많은 메모리가 필요합니다.\n",
    "\n",
    "- 적합한 상황\n",
    "    - 충분한 계산 자원이 있을 때\n",
    "    - 새로운 작업이 사전 훈련된 작업과 상당히 다를 때\n",
    "    - 대규모의 fine-tuning 데이터셋이 있을 때\n",
    "\n",
    "### 일부 레이어 고정 (Partial Fine-tuning)\n",
    "\n",
    "- 특징\n",
    "    - 일부 레이어(주로 하위 레이어)를 고정하고 나머지 레이어만 업데이트합니다.\n",
    "    - 보통 초기 레이어는 일반적인 특징을, 후기 레이어는 작업 특정적 특징을 학습한다는 가정에 기반합니다.\n",
    "\n",
    "- 장점\n",
    "    - 효율성: 더 적은 파라미터를 업데이트하므로 계산 효율성이 높습니다.\n",
    "    - 과적합 감소: 학습 가능한 파라미터 수를 줄여 과적합 위험을 낮춥니다.\n",
    "    - 빠른 학습: 업데이트할 파라미터가 적어 학습 속도가 빠릅니다.\n",
    "\n",
    "- 단점\n",
    "    - 제한된 유연성: 고정된 레이어의 정보는 새 작업에 완전히 적응하지 못할 수 있습니다.\n",
    "    - 성능 제한: 전체 fine-tuning에 비해 성능이 낮을 수 있습니다.\n",
    "\n",
    "- 적합한 상황\n",
    "    - 계산 자원이 제한적일 때\n",
    "    - 새로운 작업이 사전 훈련된 작업과 유사할 때\n",
    "    - fine-tuning 데이터셋이 작을 때\n",
    "\n",
    "### 선택 기준\n",
    "- 데이터셋 크기: 작은 데이터셋에서는 일부 레이어 고정이 유리할 수 있습니다.\n",
    "- 작업 유사성: 사전 훈련 작업과 유사할수록 더 많은 레이어를 고정할 수 있습니다.\n",
    "- 계산 자원: 제한된 자원에서는 일부 레이어 고정이 효과적입니다.\n",
    "- 성능 요구사항: 최고 성능이 필요하면 전체 fine-tuning을 고려하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/joyuiyeong/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3821f9dfa44374b2ab666312caa059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b94362c10b64cd6a4e1afe076a66944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc050dae904c0d86cd2cec5978e3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d0e429ddcb4cb8a65072e1646633e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyuiyeong/.pyenv/versions/3.11.9/envs/deeplearning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\", \"tokenizer\", \"distilbert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n",
    "- 영화 리뷰 데이터셋인 imdb 를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "MAX_TOKEN_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_imdb(batch):\n",
    "    texts, labels = [], []\n",
    "    for row in batch:\n",
    "        texts.append(row[\"text\"])\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    texts = torch.LongTensor(\n",
    "        tokenizer(texts, padding=True, truncation=True, max_length=MAX_TOKEN_LEN)\n",
    "    )\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_imdb\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_imdb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Model\n",
    "- 사전 학습된 Distil BERT 를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "- [논문](https://aclanthology.org/N19-1423.pdf)\n",
    "- BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- BERT는 2018년 Google AI Language 팀이 개발한 혁신적인 자연어 처리 모델입니다.\n",
    "- 전이 학습으로 사용되는 모델로서 만들어졌습니다.\n",
    "\n",
    "![BERT](./img/BERT.png)\n",
    "\n",
    "### 주요 특징\n",
    "- 양방향성 (Bidirectional): 문장의 전후 컨텍스트를 동시에 고려합니다.\n",
    "- 트랜스포머 기반: 어텐션 메커니즘을 활용하여 입력 시퀀스의 모든 단어 간 관계를 학습합니다.\n",
    "- 사전 학습 및 미세 조정: 대규모 텍스트 코퍼스로 사전 학습 후, 특정 작업에 맞게 미세 조정됩니다.\n",
    "\n",
    "### 양방향 트랜스포머 (Bidirectional Transformer)\n",
    "- 기존 트랜스포머: 주로 왼쪽에서 오른쪽으로 (또는 그 반대로) 단방향 처리\n",
    "- BERT의 양방향 접근: 문맥을 더 정확히 이해하고 풍부한 언어 표현을 생성\n",
    "- 구현 방식: Masked Language Model (MLM)과 Next Sentence Prediction (NSP) 태스크 활용\n",
    "\n",
    "\n",
    "### 상세 설명\n",
    "1. 입력 처리\n",
    "   - WordPiece 토큰화로 단어를 하위 단위로 분할\n",
    "   - 특수 토큰 추가: [CLS] (분류), [SEP] (문장 구분)\n",
    "\n",
    "2. 임베딩 레이어\n",
    "   - 토큰 임베딩: 각 토큰을 벡터로 변환\n",
    "   - 위치 임베딩: 토큰의 순서 정보 추가\n",
    "   - 세그먼트 임베딩: 문장 구분 정보 추가\n",
    "\n",
    "3. 트랜스포머 인코더 레이어\n",
    "   - 멀티헤드 셀프 어텐션: 토큰 간 관계 학습\n",
    "   - 피드포워드 네트워크: 비선형 변환 적용\n",
    "   - 레이어 정규화 및 잔차 연결: 학습 안정화\n",
    "\n",
    "4. 출력 표현\n",
    "   - 각 토큰에 대한 컨텍스트화된 표현 생성\n",
    "   - [CLS] 토큰: 전체 입력의 요약 정보 포함\n",
    "\n",
    "### BERT의 장점\n",
    "- 심층적인 문맥 이해로 높은 성능 달성\n",
    "- 다양한 NLP 태스크에 범용적으로 적용 가능\n",
    "- 전이 학습을 통한 효율적인 모델 개발 및 적용\n",
    "\n",
    "### BERT의 한계\n",
    "- 대규모 모델로 인한 높은 계산 비용\n",
    "- 긴 시퀀스 처리에 제한 (일반적으로 512 토큰)\n",
    "- 사전 학습 데이터에 따른 편향 가능성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation 이란?\n",
    "- 정의: 큰 모델(교사 모델)의 지식을 작은 모델(학생 모델)로 전달하는 기술\n",
    "- 목적: 모델의 크기와 계산 비용을 줄이면서 성능을 유지\n",
    "\n",
    "### 대표적인 지식 증류 방법 2가지\n",
    "\n",
    "1. Soft Target Distillation\n",
    "   - 교사 모델의 소프트맥스 출력(확률 분포)을 학생 모델이 모방하도록 학습\n",
    "   - 온도 파라미터를 사용하여 소프트맥스 분포를 조절\n",
    "   - 장점: 클래스 간 관계 정보를 전달할 수 있음\n",
    "\n",
    "2. Feature-based Distillation\n",
    "   - 교사 모델의 중간 층 특징을 학생 모델이 학습하도록 함\n",
    "   - 학생 모델의 대응되는 층에서 교사 모델의 특징과 유사해지도록 학습\n",
    "   - 장점: 모델의 내부 표현을 더 효과적으로 전달할 수 있음\n",
    "\n",
    "### 지식 증류의 이점\n",
    "- 모델 경량화: 작은 모델로 비슷한 성능 달성\n",
    "- 추론 속도 향상: 경량화된 모델로 더 빠른 예측 가능\n",
    "- 리소스 효율성: 메모리 사용량 감소, 배터리 소모 감소 등\n",
    "\n",
    "- 모든 LLM 에 대해서 지식 증류를 할 수 있다면 좋겠지만, 사실상 증류 하는 것 자체가 매우 어려운 문제이기 때문에 현재는 시도를 많이 하고 있을 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilled BERT\n",
    "- DistilBERT는 BERT의 경량화 버전으로, 지식 증류 기법을 사용하여 개발됨\n",
    "- 원래 BERT 모델의 40% 크기로 축소되었지만, 성능의 97%를 유지함\n",
    "\n",
    "### 주요 특징\n",
    "1. 모델 구조\n",
    "   - BERT-base의 12개 레이어를 6개로 줄임\n",
    "   - 토큰 타입 임베딩을 제거하여 더욱 경량화\n",
    "\n",
    "2. 학습 방법\n",
    "   - 교사-학생 모델 구조를 사용한 지식 증류\n",
    "   - BERT-base를 교사 모델로 사용\n",
    "\n",
    "3. 손실 함수\n",
    "   - 언어 모델링 손실\n",
    "   - 지식 증류 손실 (소프트 타겟)\n",
    "   - 코사인 임베딩 손실\n",
    "\n",
    "### BERT와 비교한 장점\n",
    "1. 모델 크기: BERT-base의 40% 크기로, 메모리 사용량 감소\n",
    "2. 추론 속도: 약 60% 더 빠른 추론 속도\n",
    "3. 에너지 효율성: 작은 모델 크기로 인한 전력 소비 감소\n",
    "4. 모바일/엣지 디바이스 적합성: 경량화로 인해 리소스 제한적인 환경에서 사용 가능\n",
    "\n",
    "### BERT와 비교한 단점\n",
    "1. 성능 저하: BERT-base 대비 약 3%의 성능 손실\n",
    "2. 복잡한 태스크 처리: 매우 복잡하거나 미세한 언어 이해가 필요한 태스크에서는 성능 차이가 더 클 수 있음\n",
    "3. 전이 학습 능력: 더 작은 모델 크기로 인해 새로운 도메인이나 태스크로의 전이 학습 능력이 다소 제한될 수 있음\n",
    "\n",
    "### 활용 분야\n",
    "- 리소스 제한적인 환경 (모바일 기기, 엣지 컴퓨팅)\n",
    "- 실시간 처리가 필요한 애플리케이션\n",
    "- 대규모 배포가 필요한 산업용 애플리케이션\n",
    "\n",
    "\n",
    "- DistilBERT는 BERT의 성능을 대부분 유지하면서 크기와 속도 면에서 큰 개선을 이루어, 실제 애플리케이션에서 널리 사용되고 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
