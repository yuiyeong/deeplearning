{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 데이터셋으로 학습한 영화 리뷰 GPT 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyuiyeong/.pyenv/versions/3.11.9/envs/deeplearning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face datasets 라이브러리에서 IMDB 데이터셋을 로드\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# 새로운 WordPiece 토크나이저 초기화\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# BERT와 유사한 정규화 설정 (소문자 변환 및 기타 텍스트 조정)\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "\n",
    "# BERT와 유사한 사전 토크나이징 설정 (텍스트를 단어로 분리)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    # 데이터셋에서 텍스트 데이터의 청크를 생성하는 제너레이터 함수\n",
    "    for i in range(0, len(ds[\"train\"]), 1000):\n",
    "        yield ds[\"train\"][i : i + 1000][\"text\"]\n",
    "\n",
    "\n",
    "# 토크나이저를 위한 특수 토큰 정의\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "# 지정된 어휘 크기와 특수 토큰으로 토크나이저를 훈련시키기 위한 WordPieceTrainer 초기화\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, special_tokens=special_tokens)\n",
    "\n",
    "# 훈련된 토크나이저를 사용하여 텍스트 데이터로 훈련\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "\n",
    "# Hugging Face Transformers 라이브러리와 호환되는 형식으로 훈련된 토크나이저로 변환\n",
    "tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "MAX_TOKEN_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_imdb(batch):\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for row in batch:\n",
    "        tokenized = tokenizer(\n",
    "            row[\"text\"], truncation=True, max_length=MAX_TOKEN_LEN\n",
    "        ).input_ids\n",
    "\n",
    "        # [1:]은 [CLS] 토큰을 제거하기 위함\n",
    "        labels.append(torch.LongTensor(tokenized[1:]))\n",
    "\n",
    "        # [:-1]은 [SEP] 토큰을 제거하기 위함\n",
    "        texts.append(torch.LongTensor(tokenized[:-1]))\n",
    "\n",
    "    # 배치 내 모든 텍스트와 label 시퀀스를 패딩하여 같은 길이로 맞춤\n",
    "    text_inputs = pad_sequence(\n",
    "        texts, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    label_inputs = pad_sequence(\n",
    "        labels, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    return text_inputs, label_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_imdb\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=BATCH_SIZE, collate_fn=collate_imdb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10_000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        # 1. Q, K, V 생성\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        # 1.1 Reshape Q, K, V\n",
    "        q = q.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2. Attention 점수 계산\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3. Mask 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            scores = scores + (mask[:, None] * -1e9)\n",
    "\n",
    "        # 4. Softmax 적용 및 Value 와 곱셈\n",
    "        attention_weights = self.softmax(scores)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        # 4.1 Transpose 및 Reshape\n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_length, self.d_model)\n",
    "        )\n",
    "\n",
    "        # 5. 최종 선형 변환\n",
    "        output = self.wo(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.multi_head_attention(x, mask)\n",
    "        x1 = self.dropout1(x1)\n",
    "        x1 = self.layer_norm1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "        return self.layer_norm2(x2 + x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, dff, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dff = dff\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(\n",
    "            positional_encoding(max_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        mask1 = (x == tokenizer.pad_token_id)[..., None]  # (batch_size, seq_len, 1)\n",
    "        mask2 = (\n",
    "            torch.tril(torch.ones(seq_len, seq_len))\n",
    "            .type(torch.ByteTensor)\n",
    "            .to(x.device)[None]\n",
    "        )\n",
    "        mask = mask1 & mask2\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.classification(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    my_device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(10001, 32)\n",
       "  (layers): ModuleList(\n",
       "    (0-4): 5 x TransformerLayer(\n",
       "      (multi_head_attention): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (wk): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (wv): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (wo): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.5, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification): Linear(in_features=32, out_features=10001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model = GPT(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=32,\n",
    "    n_heads=4,\n",
    "    n_layers=5,\n",
    "    dff=32,\n",
    "    max_len=MAX_TOKEN_LEN,\n",
    ").to(my_device)\n",
    "gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(gpt_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | 82.28s | Total Loss: 1838.3804\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    gpt_model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for t, l in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, labels = t.to(my_device), l.to(my_device)\n",
    "\n",
    "        outputs = gpt_model(inputs)\n",
    "\n",
    "        predictions = outputs.reshape(-1, len(tokenizer))\n",
    "        labels = labels.reshape(-1)\n",
    "        mask = (inputs == tokenizer.pad_token_id).reshape(-1)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss = (loss * ~mask).sum() / (~mask).sum()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:3d} | {time.time() - start_time:.2f}s | Total Loss: {total_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am the'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"I am \"\n",
    "tokens_org = tokenizer(input_text).input_ids\n",
    "tokens = torch.LongTensor(tokens_org)[None].to(my_device)\n",
    "\n",
    "last_token_pred = gpt_model(tokens)[0, -1].argmax()\n",
    "tokenizer.decode(tokens_org + [last_token_pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(device, model, start_text, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = torch.LongTensor(tokenizer.encode(start_text))[None].to(device)\n",
    "    generated_tokens = tokens[0].tolist()\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for _ in range(max_length):\n",
    "            predictions = model(tokens)\n",
    "            next_token = predictions[0, -1, :].argmax().item()\n",
    "\n",
    "            generated_tokens.append(next_token)\n",
    "            tokens = torch.cat(\n",
    "                [tokens, torch.LongTensor([[next_token]]).to(device)], dim=1\n",
    "            )\n",
    "\n",
    "            if next_token == tokenizer.sep_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how was the movie?, and the film. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie. the movie.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"how was the movie?\"\n",
    "generate_text(my_device, gpt_model, input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
