{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface 의 라이브러리를 활용해보기\n",
    "\n",
    "- 미리 설치해야하는 패키지\n",
    "    ```\n",
    "    pip install transformers datasets evaluate accelerate scikit-learn\n",
    "    ``` \n",
    "### transformers 라이브러리\n",
    "- transformers는 Hugging Face에서 제공하는 강력한 자연어 처리(NLP) 라이브러리입니다. \n",
    "- 이 라이브러리는 최신 트랜스포머 기반 모델들을 쉽게 사용할 수 있게 해줍니다.\n",
    "- 주요 특징\n",
    "    - 다양한 사전 훈련 모델: BERT, GPT, RoBERTa, T5 등 다양한 최신 모델을 제공합니다.\n",
    "    - 다국어 지원: 100개 이상의 언어에 대한 모델을 지원합니다.\n",
    "    - 다양한 NLP 태스크: 텍스트 분류, 질문 답변, 요약, 번역 등 다양한 NLP 태스크를 수행할 수 있습니다.\n",
    "    - 쉬운 사용법: 간단한 API를 통해 복잡한 모델을 쉽게 사용할 수 있습니다.\n",
    "    - 모델 미세조정: 사전 훈련된 모델을 특정 태스크에 맞게 쉽게 미세조정할 수 있습니다.\n",
    "    - PyTorch, TensorFlow, JAX 지원: 다양한 딥러닝 프레임워크와 호환됩니다.\n",
    "\n",
    "### evaluate 라이브러리 \n",
    "- evaluate는 Hugging Face에서 제공하는 파이썬 라이브러리로, 기계 학습 모델의 성능을 평가하기 위한 도구입니다.\n",
    "- 주요 특징\n",
    "    - 다양한 평가 지표: 정확도, F1 점수, BLEU 점수 등 다양한 평가 지표를 제공합니다.\n",
    "    - 간편한 사용: 간단한 API를 통해 쉽게 평가 지표를 계산할 수 있습니다.\n",
    "    - 확장성: 사용자 정의 평가 지표를 쉽게 추가할 수 있습니다.\n",
    "    - 다국어 지원: 여러 언어에 대한 평가를 지원합니다.\n",
    "    - 데이터셋 호환성: Hugging Face의 datasets 라이브러리와 잘 통합됩니다.\n",
    "\n",
    "### accelerate 라이브러리\n",
    "- accelerate는 Hugging Face에서 제공하는 파이썬 라이브러리로, 딥러닝 모델의 학습 과정을 가속화하고 분산 학습을 쉽게 구현할 수 있게 해주는 도구입니다.\n",
    "- 주요 특징\n",
    "    - 분산 학습 지원: 여러 GPU나 TPU를 사용한 분산 학습을 쉽게 구현할 수 있습니다.\n",
    "    - 혼합 정밀도 훈련: FP16이나 BF16과 같은 혼합 정밀도 훈련을 자동으로 처리합니다.\n",
    "    - 코드 변경 최소화: 기존 PyTorch 코드를 최소한으로 수정하여 분산 학습을 구현할 수 있습니다.\n",
    "    - 다양한 하드웨어 지원: CPU, GPU, TPU 등 다양한 하드웨어에서 동작합니다.\n",
    "    - 메모리 최적화: 그래디언트 누적, 모델 샤딩 등을 통해 메모리 사용을 최적화합니다.\n",
    "    - 쉬운 사용법: 간단한 데코레이터나 래퍼 함수를 사용하여 코드를 가속화할 수 있습니다.\n",
    "- accelerate 라이브러리를 사용하면 복잡한 분산 학습 설정 없이도 효율적인 모델 학습이 가능합니다.\n",
    "- 특히 대규모 언어 모델과 같은 큰 모델을 학습할 때 매우 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTokenizer\n",
    "- AutoTokenizer는 Hugging Face의 transformers 라이브러리에서 제공하는 클래스입니다.\n",
    "- 이 클래스는 다양한 사전 훈련된 모델에 대한 토크나이저를 자동으로 로드하고 사용할 수 있게 해줍니다.\n",
    "- 주요 특징\n",
    "  - 모델에 맞는 토크나이저 자동 선택: 모델 이름만 제공하면 해당 모델에 적합한 토크나이저를 자동으로 로드합니다.\n",
    "  - 다양한 토크나이저 지원: BERT, GPT-2, RoBERTa 등 다양한 모델의 토크나이저를 지원합니다.\n",
    "  - 간편한 사용: `from_pretrained()` 메서드를 통해 사전 훈련된 토크나이저를 쉽게 불러올 수 있습니다.\n",
    "  - 텍스트 전처리: 입력 텍스트를 모델이 이해할 수 있는 형태로 변환합니다 (토큰화, 패딩, 트런케이션 등).\n",
    "  - 특수 토큰 처리: [CLS], [SEP] 등의 특수 토큰을 자동으로 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joyuiyeong/.pyenv/versions/3.11.9/envs/deeplearning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# imdb 데이터셋은 영어로 작성된 영화 리뷰이다.\n",
    "# uncased 대신 cased 를 사용하는 이유는,\n",
    "# 영어에서 대문자로 작성된 단어의 경우 강조를 나타내거나 ex(I LOVE IT.)\n",
    "# 고유 명사를 나타내는 경우가 있기 때문이다.\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def preprocess_function(data):\n",
    "    return bert_tokenizer(data[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "imdb_tokenized = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 output 을 보면, input_ids, attention_mask, token_type_ids 이 추가 되었음을 알 수 있습니다.\n",
    "- 이는 Tokenizer 로 text 를 token 화하고 정수 index 로 변환한 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning Data 로 부터 Validation Data 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000, 25000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20% 의 데이터를 validation data 로 사용\n",
    "# HuggingFace 의 Dataset 클래스에는 train_test_split 메서드가 있다.\n",
    "imdb_split = imdb_tokenized[\"train\"].train_test_split(test_size=0.2)\n",
    "imdb_train, imdb_val = imdb_split[\"train\"], imdb_split[\"test\"]\n",
    "imdb_test = imdb_tokenized[\"test\"]\n",
    "\n",
    "len(imdb_train), len(imdb_val), len(imdb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 구현\n",
    "- Transformer 의 다양한 모듈을 활용해서 Transformer 모델을 구현해볼 수 있습니다.\n",
    "\n",
    "### AutoModelForSequenceClassification\n",
    "- 이 클래스는 시퀀스 분류 작업을 위한 사전 훈련된 모델을 자동으로 로드하는 데 사용됩니다.\n",
    "- 시퀀스 분류 작업이란 주어진 텍스트를 미리 정의된 카테고리로 분류하는 작업을 말합니다.\n",
    "    - 예시\n",
    "        - 감성 분석: 영화 리뷰를 긍정/부정으로 분류\n",
    "        - 주제 분류: 뉴스 기사를 스포츠, 정치, 경제 등으로 분류\n",
    "        - 스팸 탐지: 이메일을 스팸/정상으로 분류\n",
    "- 다양한 모델 아키텍처(BERT, RoBERTa, DistilBERT 등)에 대해 동일한 인터페이스를 제공합니다.\n",
    "- 입력 시퀀스를 받아 지정된 클래스에 대한 로짓(logits)을 출력합니다.\n",
    "- fine-tuning에 바로 사용할 수 있어 편리합니다.\n",
    "\n",
    "### BertConfig\n",
    "- BERT 모델의 구성을 정의하는 클래스입니다.\n",
    "- 이 클래스는 transformers 패키지의 PretrainedConfig 클래스를 상속받습니다.\n",
    "- 모델의 하이퍼파라미터(레이어 수, 히든 크기, 어텐션 헤드 수 등)를 지정할 수 있습니다.\n",
    "- 사전 훈련된 모델의 구성을 로드하거나 새로운 구성을 만들 때 사용됩니다.\n",
    "- 모델의 아키텍처를 세밀하게 제어할 수 있어 커스텀 모델 생성에 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 64, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 64)\n",
       "      (token_type_embeddings): Embedding(2, 64)\n",
       "      (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertConfig\n",
    "\n",
    "# BERT 모델의 다양한 설정값을 담고 있는 BertConfig 인스턴스를 생성합니다.\n",
    "config = BertConfig()\n",
    "\n",
    "# BERT 모델의 각 layer 에서 사용하는 hidden dimension 의 크기를 설정합니다.\n",
    "# hidden_size 는 BERT 층에서 생성되는 출력 백터의 차원을 결정합니다.\n",
    "# 일반적으로 더 작은 hidden_size(예: 64)는 모델의 복잡성과 연산 비용을 줄여주지만,\n",
    "# 복잡한 패턴을 학습할 수 있는 모델의 용량도 함께 줄어듭니다.\n",
    "# (일반적으로 BERT-base 모델은 hidden_size 가 768 입니다.)\n",
    "config.hidden_size = 64  # BERT layer 의 기본 hidden dimension\n",
    "\n",
    "# Feed-forward network (FFN) layer 의 중간 hidden dimension 의 크기를 설정합니다.\n",
    "# FFN 은 어텐션 출력값을 처리하기 위한 네트워크입니다.\n",
    "# 일반적으로 hidden_size 의 4배로 설정하는데, 여기서는 훈련 속도 목적으로 hidden_size 와 같은 값으로 설정합니다.\n",
    "config.intermediate_size = 64  # FFN layer 의 중간 hidden dimension\n",
    "\n",
    "# BERT 모델의 Transformer layer 수를 설정합니다.\n",
    "# 일반적으로 BERT-base 모델은 12개의 Transformer layer 를 가지고, BERT-large 모델은 24개의 Transformer layer 를 가집니다.\n",
    "config.num_hidden_layers = 2  # BERT layer 의 기본 Transformer layer 수\n",
    "\n",
    "# Multi-head attention 에서 사용하는 head 수를 설정합니다.\n",
    "# hidden_size 로 나누어 떨어지는 값으로 설정합니다.\n",
    "config.num_attention_heads = 4  # Multi-head attention 에서 사용하는 head 수\n",
    "\n",
    "# 최종 분류 문제의 클래스 개수를 설정합니다. 여기서는 이진 분류(긍정/부정)로 2를 설정합니다.\n",
    "config.num_labels = 2  # 마지막에 예측해야하는 분류 문제의 class 개수\n",
    "\n",
    "# 생성한 설정값을 기반으로 AutoModelForSequenceClassification 를 사용하여, BERT 모델을 생성합니다.\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainingArguments 와 Trainer\n",
    "\n",
    "- TrainingArguments와 Trainer는 Hugging Face의 transformers 라이브러리에서 제공하는 중요한 클래스입니다.\n",
    "- 이들은 모델 훈련을 쉽게 설정하고 실행할 수 있게 해줍니다.\n",
    "\n",
    "### TrainingArguments\n",
    "- 모델 훈련에 필요한 다양한 매개변수를 설정하는 클래스입니다.\n",
    "- 학습률, 배치 크기, 에폭 수, 저장 경로 등 훈련과 관련된 거의 모든 설정을 제어할 수 있습니다.\n",
    "- 기본값이 제공되지만, 사용자가 필요에 따라 쉽게 커스터마이즈할 수 있습니다.\n",
    "\n",
    "### Trainer\n",
    "- 실제 모델 훈련을 담당하는 클래스입니다.\n",
    "- 모델, 데이터셋, TrainingArguments 등을 입력받아 훈련을 수행합니다.\n",
    "- 훈련 루프, 평가, 모델 저장 등의 복잡한 과정을 자동화합니다.\n",
    "- 커스텀 훈련 로직이 필요한 경우 상속을 통해 확장할 수 있습니다.\n",
    "\n",
    "### 사용 예시\n",
    "```\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # 결과물을 저장할 디렉토리\n",
    "    num_train_epochs=3,               # 총 훈련 에폭 수\n",
    "    per_device_train_batch_size=16,   # 장치당 훈련 배치 크기\n",
    "    per_device_eval_batch_size=64,    # 장치당 평가 배치 크기\n",
    "    warmup_steps=500,                 # 웜업 스텝 수\n",
    "    weight_decay=0.01,                # 가중치 감쇠\n",
    "    logging_dir=\"./logs\",             # 로그를 저장할 디렉토리\n",
    "    logging_steps=10,                 # 로깅 간격 (스텝 단위)\n",
    ")\n",
    "\n",
    "# Trainer 초기화 및 훈련\n",
    "trainer = Trainer(\n",
    "    model=model,                      # 훈련할 모델\n",
    "    args=training_args,               # 훈련 인자\n",
    "    train_dataset=train_dataset,      # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset,        # 평가 데이터셋\n",
    "    compute_metrics=compute_metrics,  # 평가 메트릭 계산 함수 (선택사항)\n",
    ")\n",
    "\n",
    "# 훈련 시작\n",
    "trainer.train()\n",
    "```\n",
    "- 이렇게 설정하면 Trainer가 자동으로 훈련을 수행하고, 지정된 간격마다 평가와 체크포인트 저장을 수행합니다.\n",
    "- 또한 훈련 중 로그를 기록하여 진행 상황을 모니터링할 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/ex_huggingface01\",  # 모델, log 등을 저장할 directory\n",
    "    num_train_epochs=10,  # 총 훈련 epoch 수\n",
    "    per_device_train_batch_size=128,  # 각 device 당 training data 의 batch size\n",
    "    per_device_eval_batch_size=128,  # 각 device 당 validation data 의 batch size\n",
    "    logging_strategy=\"epoch\",  # 로그 저장 주기로, epoch 이 끝날 때마다 training loss 등을 logging\n",
    "    do_train=True,  # 학습을 진행하겠다는 의미\n",
    "    do_eval=True,  # 학습 중간에 validation data 에 대한 평가를 수행하겠다는 의미\n",
    "    eval_strategy=\"epoch\",  # 평가 저장 주기로, epoch 이 끝날 때마다 validation loss 등을 logging\n",
    "    save_strategy=\"epoch\",  # 모델 저장 주기로, epoch 이 끝날 때마다 모델을 저장\n",
    "    learning_rate=1e-3,  # optimizer 의 learning rate\n",
    "    load_best_model_at_end=True,  # 학습이 끝난 후, validation data 에 대한 성능이 가장 좋은 모델을 채택하겠다는 의미\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    # 예측값 중 가장 높은 확률을 가진 클래스의 인덱스를 찾습니다.\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # accuracy.compute 를 사용하여 예측값과 실제 레이블 간의 정확도를 계산하여 반환합니다.\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStoppingCallback\n",
    "- EarlyStoppingCallback은 Transformers 라이브러리에서 제공하는 콜백 함수입니다.\n",
    "- 이 콜백은 모델의 성능이 일정 기간 동안 개선되지 않을 때 훈련을 조기에 중단하는 기능을 합니다.\n",
    "- 주요 파라미터\n",
    "    - early_stopping_patience: 성능 개선이 없을 때 기다리는 에폭 수\n",
    "    - early_stopping_threshold: 성능 개선으로 간주할 최소 변화량\n",
    "- 사용 예\n",
    "    ```\n",
    "    from transformers import EarlyStoppingCallback\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=3,  # 3 에폭 동안 개선이 없으면 중단\n",
    "        early_stopping_threshold=0.01  # 0.01 이상의 성능 향상만 개선으로 간주\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=imdb_train,\n",
    "    eval_dataset=imdb_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d75da3a734744b293dad92e804c273e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4709, 'grad_norm': 1.1658211946487427, 'learning_rate': 0.0009000000000000001, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6fb7f3bc4c46e998b1c0578b4f6f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2985529601573944, 'eval_accuracy': 0.8762, 'eval_runtime': 3.2226, 'eval_samples_per_second': 1551.55, 'eval_steps_per_second': 12.412, 'epoch': 1.0}\n",
      "{'loss': 0.2086, 'grad_norm': 2.3631508350372314, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cf6dae4f43480080c5b7168c7fe7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32129716873168945, 'eval_accuracy': 0.8798, 'eval_runtime': 3.1369, 'eval_samples_per_second': 1593.946, 'eval_steps_per_second': 12.752, 'epoch': 2.0}\n",
      "{'train_runtime': 69.1342, 'train_samples_per_second': 2892.924, 'train_steps_per_second': 22.709, 'train_loss': 0.33977829271061405, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=314, training_loss=0.33977829271061405, metrics={'train_runtime': 69.1342, 'train_samples_per_second': 2892.924, 'train_steps_per_second': 22.709, 'total_flos': 6739968000000.0, 'train_loss': 0.33977829271061405, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1bf22ac0c54fc4ae20939150a0d1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3144468367099762,\n",
       " 'eval_accuracy': 0.8662,\n",
       " 'eval_runtime': 15.3296,\n",
       " 'eval_samples_per_second': 1630.835,\n",
       " 'eval_steps_per_second': 12.786,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
