{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear Regression\n",
    "\n",
    "- linear regression 에 대한 gradient descent 를 직접 구현해본다.\n",
    "    - OR 문제\n",
    "    - XOR 문제\n",
    "- 손실 함수의 최솟값인(실제와 예측의 차이가 최소가 됐다는 뜻) 지점의 가중치를 찾고 싶으니까 경사 하강법을 사용해서 손실을 줄여 나가는 것\n",
    "    - 조금 더 상세히:\n",
    "        1. 목표: 손실 함수의 최솟값 찾기\n",
    "           - 이 최솟값에서 모델의 예측과 실제 값의 차이가 가장 작습니다.\n",
    "        2. 방법: 경사 하강법 사용\n",
    "           - 손실 함수의 기울기(그래디언트)를 계산합니다.\n",
    "           - 이 기울기를 따라 조금씩 내려가면서 손실을 줄여나갑니다.\n",
    "        3. 과정:\n",
    "           - 현재 가중치에서 손실 함수의 기울기를 계산합니다.\n",
    "           - 기울기의 반대 방향으로 가중치를 조금씩 조정합니다.\n",
    "           - 이 과정을 반복하여 점진적으로 손실을 줄여나갑니다.\n",
    "        4. 결과:\n",
    "           - 이 과정을 통해 손실 함수가 최소가 되는 지점의 가중치를 찾게 됩니다.\n",
    "           - 이 가중치를 사용하면 모델의 예측이 실제 값과 가장 가깝게 됩니다.\n",
    "        - 이 접근 방식의 핵심은 \"점진적 개선\"입니다. 한 번에 최적의 가중치를 찾는 것이 아니라, 조금씩 개선해 나가면서 최적점에 접근하는 것입니다.\n",
    "        - 예를 들어, 선형 회귀에서:\n",
    "            1. 처음에는 예측이 실제 값과 많이 다를 수 있습니다.\n",
    "            2. 경사 하강법을 통해 가중치를 조정합니다.\n",
    "            3. 조정된 가중치로 다시 예측을 합니다.\n",
    "            4. 새 예측은 이전보다 실제 값에 더 가까워집니다.\n",
    "            5. 이 과정을 반복하면서 점점 더 좋은 예측을 하게 됩니다.\n",
    "        \n",
    "        - 이 방법은 복잡한 모델(예: 딥러닝)에서도 동일하게 적용되며, 대부분의 머신러닝 알고리즘의 학습 과정에서 핵심적인 역할을 합니다."
   ],
   "id": "fe637c2758367a19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 경사 하강법(Gradient Descent)\n",
    "- 최적화 알고리즘으로, 함수의 최소값을 찾는 데 사용\n",
    "- 머신러닝에서는 주로 손실 함수를 최소화하는 모델 파라미터를 찾는 데 사용됩니다.\n",
    "### 경사 하강법의 기본 아이디어와 작동 방식\n",
    "1. 현재 위치에서 함수의 기울기(그래디언트)를 계산합니다.\n",
    "2. 기울기의 반대 방향으로 작은 스텝을 이동합니다.\n",
    "3. 새로운 위치에서 1, 2를 반복합니다.\n",
    "4. 기울기가 0에 가까워지거나 정해진 반복 횟수에 도달하면 종료합니다.\n",
    "\n",
    "### 예시: 1차원 함수 최소화\n",
    "간단한 1차 함수 f(x) = x^2 + 2를 최소화하는 예를 들어보겠습니다.\n",
    "\n",
    "1. 초기화\n",
    "    - 시작점 x = 5 (임의의 값)\n",
    "    - 학습률(learning rate) α = 0.1\n",
    "2. 반복\n",
    "    a) 그래디언트 계산: f'(x) = 2x\n",
    "    b) x 업데이트: x = x - α * f'(x)\n",
    "1회차:\n",
    "    f'(5) = 2 * 5 = 10\n",
    "    x = 5 - 0.1 * 10 = 4\n",
    "\n",
    "2회차:\n",
    "    f'(4) = 2 * 4 = 8\n",
    "    x = 4 - 0.1 * 8 = 3.2\n",
    "\n",
    "... (반복)\n",
    "10회차:\n",
    "    f'(0.5242) ≈ 1.0484\n",
    "    x ≈ 0.4193\n",
    "\n",
    "이런 식으로 계속 반복하면 x는 0에 수렴하게 됩니다 (f(x)의 최소점).\n",
    "\n",
    "### 실제 머신러닝에서의 적용\n",
    "선형 회귀를 예로 들어보겠습니다.\n",
    "목표는 MSE(평균 제곱 오차)를 최소화하는 가중치 w와 편향 b를 찾는 것입니다.\n",
    "\n",
    "1. 초기화\n",
    "    - w와 b를 랜덤한 작은 값으로 초기화\n",
    "    - 학습률 α 설정 (예: 0.01)\n",
    "2. 반복\n",
    "    a) 예측: ŷ = wx + b\n",
    "    b) 손실 계산: MSE = (1/n) * Σ(y - ŷ)^2\n",
    "    c) 그래디언트 계산:\n",
    "        ∂MSE/∂w = (2/n) * Σ(wx + b - y)x\n",
    "        ∂MSE/∂b = (2/n) * Σ(wx + b - y)\n",
    "    d) 파라미터 업데이트:\n",
    "        w = w - α * (∂MSE/∂w)\n",
    "        b = b - α * (∂MSE/∂b)\n",
    "3. 종료 조건 확인\n",
    "    - 손실이 충분히 작아졌거나\n",
    "    - 그래디언트가 거의 0이 되었거나\n",
    "    - 최대 반복 횟수에 도달"
   ],
   "id": "7c35b48cf88c2c6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.284901Z",
     "start_time": "2024-09-10T09:00:19.806002Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "6b4f8338042af716",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OR 문제\n",
    "- 0 or 0 -> 0\n",
    "- 0 or 1 -> 1\n",
    "- 1 or 0 -> 1\n",
    "- 1 or 1 -> 1"
   ],
   "id": "cac34fb1bd3e6325"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.295878Z",
     "start_time": "2024-09-10T09:00:20.293458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor(\n",
    "    [\n",
    "        [0.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [1.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "y = torch.tensor([0, 1, 1, 1])\n",
    "print(x.shape, y.shape)"
   ],
   "id": "cddf2378586ff748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- linear regression 의 $w, b$ 를 정의한다.\n",
    "- 우리가 찾을 식은 b + $wi*xi + wi-1*xi-1 + ... + w0*x0 = y$\n",
    "- 이 문제에 맞게 표현하면, $b + \\mathbf{w}^T \\mathbf{x} = y$\n",
    "        $$\n",
    "        \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}, \\quad\n",
    "        \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "        $$ \n",
    "- 즉 벡터 $w$ 와 스칼라 $b$ 를 찾는 것\n"
   ],
   "id": "44e84f4bb94a1ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 학습한 모델의 성능을 측정\n",
    "- 왜냐하면, 잘 학습을 했는지 잘못 학습을 했는지 판별해야하니까. \n",
    "    - 잘못 학습을 했다면, 내용을 바꿔서 다시 학습해야죠.\n",
    "- Linear Regression 에서는 성능 평가 함수(loss function)로 주로 MSE(Mean Squared Error) 를 사용한다.\n",
    "    - MSE: 예측값과 실제값의 차이의 제곱을 평균한 값.\n",
    "        - 이것을 사용하는 이유는 다음과 같다.\n",
    "            - 제곱을 사용함으로써, 오차의 방향을 무시(음수 오차든 양수 오차든 동등하게 취급)\n",
    "            - 제곱을 사용함으로써, 큰 오차일 수록 큰 가중치가 적용되는 것\n",
    "                - 예를들어, 오차가 2면 제곱일 때 4고, 오차가 3이면 제곱일 때 9인데 평균을 내니까 오차가 3인 것의 가중치가 큰 것\n",
    "            - 미분이 가능해서 최적화 알고리즘을 사용하기에 적합하다.\n",
    "            - 단위가 원래 변수의 제곱이라서 직관적으로 이해하기가 쉽다.\n",
    "            - 통계적으로 가우시안 노이즈를 가정할 때 최대 우도 추정과 관련이 있다는데 이건 무슨 소리인지 모르겠음\n",
    "    $$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "    - $n$은 데이터 포인트의 수\n",
    "    - $y_i$는 실제값\n",
    "    - $\\hat{y}_i$는 예측값"
   ],
   "id": "484abcead294cb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.399086Z",
     "start_time": "2024-09-10T09:00:20.397200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(w, b, x):\n",
    "    return torch.matmul(x, w.T) + b  # (N, 1) shape 인 tensor\n",
    "\n",
    "\n",
    "def loss(y, pred):\n",
    "    # torch.nn.MSELoss() 와 같음\n",
    "    return (\n",
    "        (y - pred).pow(2).mean()\n",
    "    )  # (N, 1) 과 (N, 1) 에 대해서 연산 진행 후 scalar 반환"
   ],
   "id": "978054a5fc7a0789",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- MSE 기반으로 $w$ 와 $b$ 의 gradient 를 구하는 수식은 아래와 같음\n",
    "- $w$의 gradient\n",
    "$$\\frac{\\partial l}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y)x_i.$$\n",
    "    - $\\frac{\\partial l}{\\partial w}$: MSE 손실 함수 l에 대한 w의 편미분\n",
    "    - $\\frac{1}{n}$: n개의 샘플에 대한 평균\n",
    "    - $\\sum_{i=1}^n$: 모든 샘플에 대해 합산\n",
    "    - $wx_i^T + b$: i번째 샘플에 대한 예측값\n",
    "    - $y_i$: i번째 샘플의 실제값\n",
    "    - $(wx_i^T + b - y_i)$: 예측값과 실제값의 차이 (오차)\n",
    "    - $2(wx_i^T + b - y_i)$: MSE의 미분 결과로 나오는 2를 곱한 항\n",
    "    - $x_i$: 오차와 입력을 곱함 (체인 룰 적용)\n",
    "\n",
    "- $b$에 대한 gradient\n",
    "$$\\frac{\\partial l}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y).$$\n",
    "    - $\\frac{\\partial l}{\\partial b}$: MSE 손실 함수 l에 대한 b의 편미분\n",
    "    - 나머지 항목들은 w의 그래디언트와 동일한 의미를 가집니다.\n",
    "    - w의 그래디언트와 다른 점은 마지막에 $x_i$를 곱하지 않는다는 것입니다."
   ],
   "id": "dc918a123e9768ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.405569Z",
     "start_time": "2024-09-10T09:00:20.403692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cal_gradient_w(w, b, x, y):\n",
    "    tmp1 = torch.matmul(w, x.T)\n",
    "    tmp2 = tmp1 + b\n",
    "    tmp3 = 2 * (tmp2 - y[None])\n",
    "    grad_item = tmp3.T * x\n",
    "    return grad_item.mean(dim=0, keepdim=True)\n",
    "\n",
    "\n",
    "def cal_gradient_b(w, b, x, y):\n",
    "    grad_item = 2 * (torch.matmul(w, x.T) + b - y[None])\n",
    "    return grad_item.mean(dim=-1, keepdim=True)"
   ],
   "id": "6a7d97876ed0298c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent\n",
    "$$w^{(new)} = w^{(old)} - \\eta \\frac{\\partial l}{\\partial w} \\biggr\\rvert_{w = w^{(old)}}$$"
   ],
   "id": "478002f8f05a9d63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.411869Z",
     "start_time": "2024-09-10T09:00:20.410107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update(x, y, w, b, lr):\n",
    "    w = w - lr * cal_gradient_w(w, b, x, y)\n",
    "    b = b - lr * cal_gradient_b(w, b, x, y)\n",
    "    return w, b"
   ],
   "id": "2df466c9605557a7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.417668Z",
     "start_time": "2024-09-10T09:00:20.415923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(n_epochs, lr, w, b, x, y):\n",
    "    for e in range(n_epochs):\n",
    "        w, b = update(x, y, w, b, lr)\n",
    "        print(\n",
    "            \"Epoch\",\n",
    "            e,\n",
    "            \"Loss:\",\n",
    "            loss(y, predict(w, b, x)),\n",
    "        )\n",
    "    return w, b"
   ],
   "id": "2a097ce7355644bb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.424812Z",
     "start_time": "2024-09-10T09:00:20.423031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = torch.randn((1, 2))  # 1 행 2 열\n",
    "b = torch.randn((1, 1))  # 1 행 1 열\n",
    "print(w.shape, b.shape)"
   ],
   "id": "e223c04cc624e78a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.444957Z",
     "start_time": "2024-09-10T09:00:20.434280Z"
    }
   },
   "cell_type": "code",
   "source": "w, b = train(100, 0.1, w, b, x, y)",
   "id": "70ee1c05bbcce3e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: tensor(2.1933)\n",
      "Epoch 1 Loss: tensor(1.5819)\n",
      "Epoch 2 Loss: tensor(1.2778)\n",
      "Epoch 3 Loss: tensor(1.1215)\n",
      "Epoch 4 Loss: tensor(1.0355)\n",
      "Epoch 5 Loss: tensor(0.9826)\n",
      "Epoch 6 Loss: tensor(0.9450)\n",
      "Epoch 7 Loss: tensor(0.9146)\n",
      "Epoch 8 Loss: tensor(0.8876)\n",
      "Epoch 9 Loss: tensor(0.8624)\n",
      "Epoch 10 Loss: tensor(0.8383)\n",
      "Epoch 11 Loss: tensor(0.8151)\n",
      "Epoch 12 Loss: tensor(0.7928)\n",
      "Epoch 13 Loss: tensor(0.7714)\n",
      "Epoch 14 Loss: tensor(0.7508)\n",
      "Epoch 15 Loss: tensor(0.7312)\n",
      "Epoch 16 Loss: tensor(0.7124)\n",
      "Epoch 17 Loss: tensor(0.6944)\n",
      "Epoch 18 Loss: tensor(0.6774)\n",
      "Epoch 19 Loss: tensor(0.6611)\n",
      "Epoch 20 Loss: tensor(0.6456)\n",
      "Epoch 21 Loss: tensor(0.6309)\n",
      "Epoch 22 Loss: tensor(0.6169)\n",
      "Epoch 23 Loss: tensor(0.6035)\n",
      "Epoch 24 Loss: tensor(0.5908)\n",
      "Epoch 25 Loss: tensor(0.5788)\n",
      "Epoch 26 Loss: tensor(0.5673)\n",
      "Epoch 27 Loss: tensor(0.5563)\n",
      "Epoch 28 Loss: tensor(0.5459)\n",
      "Epoch 29 Loss: tensor(0.5360)\n",
      "Epoch 30 Loss: tensor(0.5266)\n",
      "Epoch 31 Loss: tensor(0.5176)\n",
      "Epoch 32 Loss: tensor(0.5090)\n",
      "Epoch 33 Loss: tensor(0.5008)\n",
      "Epoch 34 Loss: tensor(0.4930)\n",
      "Epoch 35 Loss: tensor(0.4856)\n",
      "Epoch 36 Loss: tensor(0.4785)\n",
      "Epoch 37 Loss: tensor(0.4717)\n",
      "Epoch 38 Loss: tensor(0.4652)\n",
      "Epoch 39 Loss: tensor(0.4590)\n",
      "Epoch 40 Loss: tensor(0.4531)\n",
      "Epoch 41 Loss: tensor(0.4475)\n",
      "Epoch 42 Loss: tensor(0.4421)\n",
      "Epoch 43 Loss: tensor(0.4370)\n",
      "Epoch 44 Loss: tensor(0.4320)\n",
      "Epoch 45 Loss: tensor(0.4273)\n",
      "Epoch 46 Loss: tensor(0.4228)\n",
      "Epoch 47 Loss: tensor(0.4185)\n",
      "Epoch 48 Loss: tensor(0.4144)\n",
      "Epoch 49 Loss: tensor(0.4104)\n",
      "Epoch 50 Loss: tensor(0.4066)\n",
      "Epoch 51 Loss: tensor(0.4030)\n",
      "Epoch 52 Loss: tensor(0.3995)\n",
      "Epoch 53 Loss: tensor(0.3962)\n",
      "Epoch 54 Loss: tensor(0.3930)\n",
      "Epoch 55 Loss: tensor(0.3900)\n",
      "Epoch 56 Loss: tensor(0.3870)\n",
      "Epoch 57 Loss: tensor(0.3842)\n",
      "Epoch 58 Loss: tensor(0.3815)\n",
      "Epoch 59 Loss: tensor(0.3789)\n",
      "Epoch 60 Loss: tensor(0.3765)\n",
      "Epoch 61 Loss: tensor(0.3741)\n",
      "Epoch 62 Loss: tensor(0.3718)\n",
      "Epoch 63 Loss: tensor(0.3696)\n",
      "Epoch 64 Loss: tensor(0.3675)\n",
      "Epoch 65 Loss: tensor(0.3655)\n",
      "Epoch 66 Loss: tensor(0.3635)\n",
      "Epoch 67 Loss: tensor(0.3616)\n",
      "Epoch 68 Loss: tensor(0.3598)\n",
      "Epoch 69 Loss: tensor(0.3581)\n",
      "Epoch 70 Loss: tensor(0.3564)\n",
      "Epoch 71 Loss: tensor(0.3549)\n",
      "Epoch 72 Loss: tensor(0.3533)\n",
      "Epoch 73 Loss: tensor(0.3518)\n",
      "Epoch 74 Loss: tensor(0.3504)\n",
      "Epoch 75 Loss: tensor(0.3491)\n",
      "Epoch 76 Loss: tensor(0.3477)\n",
      "Epoch 77 Loss: tensor(0.3465)\n",
      "Epoch 78 Loss: tensor(0.3453)\n",
      "Epoch 79 Loss: tensor(0.3441)\n",
      "Epoch 80 Loss: tensor(0.3430)\n",
      "Epoch 81 Loss: tensor(0.3419)\n",
      "Epoch 82 Loss: tensor(0.3408)\n",
      "Epoch 83 Loss: tensor(0.3398)\n",
      "Epoch 84 Loss: tensor(0.3389)\n",
      "Epoch 85 Loss: tensor(0.3379)\n",
      "Epoch 86 Loss: tensor(0.3370)\n",
      "Epoch 87 Loss: tensor(0.3362)\n",
      "Epoch 88 Loss: tensor(0.3353)\n",
      "Epoch 89 Loss: tensor(0.3345)\n",
      "Epoch 90 Loss: tensor(0.3337)\n",
      "Epoch 91 Loss: tensor(0.3330)\n",
      "Epoch 92 Loss: tensor(0.3323)\n",
      "Epoch 93 Loss: tensor(0.3316)\n",
      "Epoch 94 Loss: tensor(0.3309)\n",
      "Epoch 95 Loss: tensor(0.3303)\n",
      "Epoch 96 Loss: tensor(0.3297)\n",
      "Epoch 97 Loss: tensor(0.3291)\n",
      "Epoch 98 Loss: tensor(0.3285)\n",
      "Epoch 99 Loss: tensor(0.3279)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.578418Z",
     "start_time": "2024-09-10T09:00:20.575445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(predict(w, b, x))\n",
    "print(y)"
   ],
   "id": "828f70777f2aa5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2152],\n",
      "        [0.7459],\n",
      "        [0.7444],\n",
      "        [1.2751]])\n",
      "tensor([0, 1, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## XOR\n",
    "- 0 xor 0 -> 0\n",
    "- 0 xor 1 -> 1\n",
    "- 1 xor 0 -> 1\n",
    "- 1 xor 1 -> 0"
   ],
   "id": "c1259929cab86f47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.628017Z",
     "start_time": "2024-09-10T09:00:20.625680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = torch.tensor([0, 1, 1, 0])\n",
    "print(x.shape, y.shape)"
   ],
   "id": "3824674d92992767",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.666003Z",
     "start_time": "2024-09-10T09:00:20.664460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# w = torch.randn((1, 2))\n",
    "# b = torch.randn((1, 1))"
   ],
   "id": "bfd5afc6a41c7d64",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.686330Z",
     "start_time": "2024-09-10T09:00:20.675473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w, b = train(100, 0.1, w, b, x, y)\n",
    "print(w, b)"
   ],
   "id": "f20f059d0d267edd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: tensor(0.3888)\n",
      "Epoch 1 Loss: tensor(0.3551)\n",
      "Epoch 2 Loss: tensor(0.3357)\n",
      "Epoch 3 Loss: tensor(0.3237)\n",
      "Epoch 4 Loss: tensor(0.3155)\n",
      "Epoch 5 Loss: tensor(0.3094)\n",
      "Epoch 6 Loss: tensor(0.3045)\n",
      "Epoch 7 Loss: tensor(0.3004)\n",
      "Epoch 8 Loss: tensor(0.2967)\n",
      "Epoch 9 Loss: tensor(0.2935)\n",
      "Epoch 10 Loss: tensor(0.2905)\n",
      "Epoch 11 Loss: tensor(0.2878)\n",
      "Epoch 12 Loss: tensor(0.2852)\n",
      "Epoch 13 Loss: tensor(0.2829)\n",
      "Epoch 14 Loss: tensor(0.2807)\n",
      "Epoch 15 Loss: tensor(0.2786)\n",
      "Epoch 16 Loss: tensor(0.2767)\n",
      "Epoch 17 Loss: tensor(0.2749)\n",
      "Epoch 18 Loss: tensor(0.2733)\n",
      "Epoch 19 Loss: tensor(0.2717)\n",
      "Epoch 20 Loss: tensor(0.2703)\n",
      "Epoch 21 Loss: tensor(0.2689)\n",
      "Epoch 22 Loss: tensor(0.2677)\n",
      "Epoch 23 Loss: tensor(0.2665)\n",
      "Epoch 24 Loss: tensor(0.2654)\n",
      "Epoch 25 Loss: tensor(0.2644)\n",
      "Epoch 26 Loss: tensor(0.2634)\n",
      "Epoch 27 Loss: tensor(0.2625)\n",
      "Epoch 28 Loss: tensor(0.2617)\n",
      "Epoch 29 Loss: tensor(0.2609)\n",
      "Epoch 30 Loss: tensor(0.2602)\n",
      "Epoch 31 Loss: tensor(0.2595)\n",
      "Epoch 32 Loss: tensor(0.2589)\n",
      "Epoch 33 Loss: tensor(0.2583)\n",
      "Epoch 34 Loss: tensor(0.2578)\n",
      "Epoch 35 Loss: tensor(0.2572)\n",
      "Epoch 36 Loss: tensor(0.2568)\n",
      "Epoch 37 Loss: tensor(0.2563)\n",
      "Epoch 38 Loss: tensor(0.2559)\n",
      "Epoch 39 Loss: tensor(0.2555)\n",
      "Epoch 40 Loss: tensor(0.2551)\n",
      "Epoch 41 Loss: tensor(0.2548)\n",
      "Epoch 42 Loss: tensor(0.2545)\n",
      "Epoch 43 Loss: tensor(0.2542)\n",
      "Epoch 44 Loss: tensor(0.2539)\n",
      "Epoch 45 Loss: tensor(0.2536)\n",
      "Epoch 46 Loss: tensor(0.2534)\n",
      "Epoch 47 Loss: tensor(0.2532)\n",
      "Epoch 48 Loss: tensor(0.2530)\n",
      "Epoch 49 Loss: tensor(0.2528)\n",
      "Epoch 50 Loss: tensor(0.2526)\n",
      "Epoch 51 Loss: tensor(0.2524)\n",
      "Epoch 52 Loss: tensor(0.2523)\n",
      "Epoch 53 Loss: tensor(0.2521)\n",
      "Epoch 54 Loss: tensor(0.2520)\n",
      "Epoch 55 Loss: tensor(0.2518)\n",
      "Epoch 56 Loss: tensor(0.2517)\n",
      "Epoch 57 Loss: tensor(0.2516)\n",
      "Epoch 58 Loss: tensor(0.2515)\n",
      "Epoch 59 Loss: tensor(0.2514)\n",
      "Epoch 60 Loss: tensor(0.2513)\n",
      "Epoch 61 Loss: tensor(0.2512)\n",
      "Epoch 62 Loss: tensor(0.2511)\n",
      "Epoch 63 Loss: tensor(0.2511)\n",
      "Epoch 64 Loss: tensor(0.2510)\n",
      "Epoch 65 Loss: tensor(0.2509)\n",
      "Epoch 66 Loss: tensor(0.2509)\n",
      "Epoch 67 Loss: tensor(0.2508)\n",
      "Epoch 68 Loss: tensor(0.2508)\n",
      "Epoch 69 Loss: tensor(0.2507)\n",
      "Epoch 70 Loss: tensor(0.2507)\n",
      "Epoch 71 Loss: tensor(0.2506)\n",
      "Epoch 72 Loss: tensor(0.2506)\n",
      "Epoch 73 Loss: tensor(0.2505)\n",
      "Epoch 74 Loss: tensor(0.2505)\n",
      "Epoch 75 Loss: tensor(0.2505)\n",
      "Epoch 76 Loss: tensor(0.2504)\n",
      "Epoch 77 Loss: tensor(0.2504)\n",
      "Epoch 78 Loss: tensor(0.2504)\n",
      "Epoch 79 Loss: tensor(0.2504)\n",
      "Epoch 80 Loss: tensor(0.2503)\n",
      "Epoch 81 Loss: tensor(0.2503)\n",
      "Epoch 82 Loss: tensor(0.2503)\n",
      "Epoch 83 Loss: tensor(0.2503)\n",
      "Epoch 84 Loss: tensor(0.2503)\n",
      "Epoch 85 Loss: tensor(0.2502)\n",
      "Epoch 86 Loss: tensor(0.2502)\n",
      "Epoch 87 Loss: tensor(0.2502)\n",
      "Epoch 88 Loss: tensor(0.2502)\n",
      "Epoch 89 Loss: tensor(0.2502)\n",
      "Epoch 90 Loss: tensor(0.2502)\n",
      "Epoch 91 Loss: tensor(0.2502)\n",
      "Epoch 92 Loss: tensor(0.2501)\n",
      "Epoch 93 Loss: tensor(0.2501)\n",
      "Epoch 94 Loss: tensor(0.2501)\n",
      "Epoch 95 Loss: tensor(0.2501)\n",
      "Epoch 96 Loss: tensor(0.2501)\n",
      "Epoch 97 Loss: tensor(0.2501)\n",
      "Epoch 98 Loss: tensor(0.2501)\n",
      "Epoch 99 Loss: tensor(0.2501)\n",
      "tensor([[0.0130, 0.0130]]) tensor([[0.4849]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T09:00:20.713738Z",
     "start_time": "2024-09-10T09:00:20.711366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(predict(w, b, x))\n",
    "print(y)"
   ],
   "id": "f719849cc297c1e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4849],\n",
      "        [0.4979],\n",
      "        [0.4979],\n",
      "        [0.5109]])\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 제대로 처리하지 못함.",
   "id": "d5577796afe5f5bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
